{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6909d528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "import os, re, datetime as dt, time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cudf                 # GPU Parquet\n",
    "import cupy as cp            # companion to cudf\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda import amp\n",
    "from sklearn.metrics import f1_score                # ①\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using\", device)\n",
    "torch.backends.cudnn.benchmark = True    # gyorsítja a fix input-méretű CNN-t\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf4c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_PRINT = True  # master switch – set False when prints are no longer needed\n",
    "\n",
    "def p(msg: str):\n",
    "    \"\"\"Wrapped print so we can globally silence if needed.\"\"\"\n",
    "    if DEBUG_PRINT:\n",
    "        print(f\"[DEBUG] {msg}\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e781c27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "# 0. Device & torch settings\n",
    "# -----------------------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using\", device)\n",
    "if device.type == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97c3a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_book_chunk(\n",
    "    start_date: dt.datetime,\n",
    "    end_date:   dt.datetime,\n",
    "    symbol: str,\n",
    "    data_dir: str = \"./szakdolgozat-high-freq-btc-prediction/data\",\n",
    "):\n",
    "    \"\"\"Load LOB parquet chunks between *start_date* and *end_date* (inclusive).\"\"\"\n",
    "    p(f\"load_book_chunk(): symbol={symbol}, range=({start_date} … {end_date})\")\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    sym_pat = symbol.lower().replace(\"-\", \"_\")\n",
    "    rex = re.compile(rf\"book_{sym_pat}_(\\d{{8}})_(\\d{{8}})\\.parquet$\", re.I)\n",
    "\n",
    "    sd, ed = pd.to_datetime(start_date), pd.to_datetime(end_date)\n",
    "    frames = []\n",
    "\n",
    "    for fn in sorted(os.listdir(data_dir)):\n",
    "        m = rex.match(fn)\n",
    "        fp = Path(data_dir) / fn\n",
    "        if not m or not fp.is_file():\n",
    "            continue\n",
    "\n",
    "        f_sd, f_ed = pd.to_datetime(m.group(1)), pd.to_datetime(m.group(2))\n",
    "        if f_ed < sd or f_sd > ed:\n",
    "            continue  # out of range\n",
    "\n",
    "        p(f\"  reading {fn}\")\n",
    "        df = cudf.read_parquet(fp)\n",
    "        msk = (df[\"received_time\"] >= sd) & (df[\"received_time\"] <= ed)\n",
    "        if bool(msk.any()):\n",
    "            p(f\"    -> {msk.sum()} rows kept from this file\")\n",
    "            frames.append(df[msk])\n",
    "        else:\n",
    "            p(\"    -> 0 rows in range – skipped\")\n",
    "\n",
    "    if not frames:\n",
    "        p(\"No data loaded – returning empty DataFrame\")\n",
    "        return cudf.DataFrame()\n",
    "\n",
    "    df = cudf.concat(frames, ignore_index=True)\n",
    "    p(f\"Concatenated {len(frames)} chunks – total rows: {len(df)} – took {time.perf_counter()-t0:.2f} s\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa8c0eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LobDataset(Dataset):\n",
    "    def __init__(self, df: cudf.DataFrame,\n",
    "                 depth:int = 10,\n",
    "                 window:int = 100,\n",
    "                 horizon:int = 100,\n",
    "                 alpha:float = 0.002,\n",
    "                 stride:int = 5):\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        self.depth, self.window = depth, window\n",
    "        self.horizon, self.alpha = horizon, alpha\n",
    "        self.stride = stride\n",
    "\n",
    "        p(f\"LobDataset: depth={depth}, window={window}, horizon={horizon}, stride={stride}\")\n",
    "\n",
    "        pat = rf'(bid|ask)_[0-9]{{1,2}}_(price|size)'\n",
    "        feat_cols = [c for c in df.columns\n",
    "                     if re.match(pat, c) and int(c.split('_')[1]) < depth]\n",
    "        p(f\"  selected {len(feat_cols)} feature columns for LOB levels\")\n",
    "\n",
    "        # ---- rolling z‑score over ~5 days (~6.12 Hz sampling) ----\n",
    "        wnd = int(5*24*60*60*6.12)\n",
    "        p(f\"  starting 5‑day z‑score normalisation (window={wnd}) on GPU …\")\n",
    "        t_norm = time.perf_counter()\n",
    "        mu  = df[feat_cols].rolling(wnd, 1).mean()\n",
    "        sig = df[feat_cols].rolling(wnd, 1).std() + 1e-8\n",
    "        df[feat_cols] = ((df[feat_cols] - mu) / sig).astype(cp.float16)\n",
    "        p(f\"  normalisation finished in {time.perf_counter()-t_norm:.2f} s\")\n",
    "\n",
    "        # ---- single torch tensor on CPU (fp16) ----\n",
    "        self.X = torch.from_dlpack(df[feat_cols].T.to_dlpack())\\\n",
    "                       .view(-1, depth*4)\\\n",
    "                       .to(torch.float16, device='cpu', non_blocking=True)\n",
    "        mid = (df[\"bid_0_price\"] + df[\"ask_0_price\"]) / 2\n",
    "        self.mid = torch.from_dlpack(mid.to_dlpack())\\\n",
    "                         .to(torch.float16, device='cpu', non_blocking=True)\n",
    "\n",
    "        p(f\"  LobDataset ready – {len(self)} samples – setup took {time.perf_counter()-t0:.2f} s\")\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def __len__(self):\n",
    "        return (len(self.X) - self.window - self.horizon) // self.stride\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = i * self.stride\n",
    "        j   = idx + self.window\n",
    "\n",
    "        x  = self.X[idx:j].view(self.window, self.depth*4)\n",
    "        r  = (self.mid[j+self.horizon-1] - self.mid[j-1]) / self.mid[j-1]\n",
    "        y  = 2 if r >  self.alpha else 0 if r < -self.alpha else 1\n",
    "        return x, torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03b79585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 – új verzió\n",
    "def make_loaders(df,\n",
    "                 depth:   int = 10,\n",
    "                 window:  int = 100,\n",
    "                 horizon: int = 100,\n",
    "                 valid_frac: float = 0.1,\n",
    "                 batch: int = 32):\n",
    "\n",
    "    ds = LobDataset(df,\n",
    "                    depth   = depth,\n",
    "                    window  = window,\n",
    "                    horizon = horizon)\n",
    "\n",
    "    n = len(ds)\n",
    "    idx = np.arange(n);  np.random.shuffle(idx)\n",
    "    split = int(n * (1 - valid_frac))\n",
    "\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(idx[:split])\n",
    "    val_sampler   = torch.utils.data.SubsetRandomSampler(idx[split:])\n",
    "\n",
    "    train_loader = DataLoader(ds,\n",
    "                              batch_size = batch,\n",
    "                              sampler    = train_sampler,\n",
    "                              num_workers= 2,\n",
    "                              prefetch_factor = 4,\n",
    "                              pin_memory = True,\n",
    "                              persistent_workers = True)\n",
    "    val_loader   = DataLoader(ds,\n",
    "                              batch_size = batch,\n",
    "                              sampler    = val_sampler,\n",
    "                              num_workers= 2,\n",
    "                              pin_memory = True,\n",
    "                              persistent_workers = True)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef048974",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionModule(nn.Module):\n",
    "    \"\"\"DeepLOB‑style Inception@32 with detailed timing prints.\"\"\"\n",
    "    def __init__(self, in_ch: int, out_ch: int = 32,\n",
    "                 ratio: tuple = (0.25, 0.375, 0.25, 0.125)):\n",
    "        super().__init__()\n",
    "        b1, b3, b5, bp = [int(out_ch * r) for r in ratio]\n",
    "\n",
    "        self.branch1 = nn.Conv1d(in_ch, b1, 1)\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, max(1, b3 // 2), 1),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            nn.Conv1d(max(1, b3 // 2), b3, 3, padding=1)\n",
    "        )\n",
    "        self.branch5 = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, max(1, b5 // 2), 1),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            nn.Conv1d(max(1, b5 // 2), b5, 5, padding=2)\n",
    "        )\n",
    "        self.branch_pool = nn.Sequential(\n",
    "            nn.MaxPool1d(3, stride=1, padding=1),\n",
    "            nn.Conv1d(in_ch, bp, 1)\n",
    "        )\n",
    "        self.act = nn.LeakyReLU(0.01, inplace=True)\n",
    "\n",
    "    def forward(self, x):               # x: (B, T, C)\n",
    "        t0 = time.perf_counter()\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        y = torch.cat((\n",
    "            self.branch1(x),\n",
    "            self.branch3(x),\n",
    "            self.branch5(x),\n",
    "            self.branch_pool(x)\n",
    "        ), dim=1)\n",
    "        y = self.act(y)\n",
    "        y = y.transpose(1, 2).contiguous()\n",
    "        p(f\"    InceptionModule forward() took {time.perf_counter()-t0:.4f} s – output {tuple(y.shape)}\")\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22f73789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 5. DeepLOB model with per‑stage timing\n",
    "# -----------------------------------------------------------------------------\n",
    "class DeepLOB(nn.Module):\n",
    "    def __init__(self, depth: int = 10):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, (1, 2), (1, 2)),\n",
    "            nn.LeakyReLU(0.01, True),\n",
    "            nn.Conv2d(32, 32, (1, 2), (1, 2)),\n",
    "            nn.LeakyReLU(0.01, True),\n",
    "            nn.Conv2d(32, 32, (1, depth), (1, 1)),\n",
    "            nn.LeakyReLU(0.01, True),\n",
    "        )\n",
    "        self.inception = InceptionModule(in_ch=32, out_ch=32)\n",
    "        self.lstm = nn.LSTM(input_size=32,\n",
    "                            hidden_size=64,\n",
    "                            batch_first=True,\n",
    "                            dropout=0.1)\n",
    "        self.head = nn.Linear(64, 3)\n",
    "\n",
    "    def forward(self, x):              # x: (B, 100, 40)\n",
    "        t_fwd = time.perf_counter()\n",
    "\n",
    "        # CNN expects (B, C=1, H=100, W=40)\n",
    "        t0 = time.perf_counter()\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.cnn(x)\n",
    "        p(f\"    CNN took {time.perf_counter()-t0:.4f} s – output {tuple(x.shape)}\")\n",
    "\n",
    "        # prepare for Inception (B, T, C)\n",
    "        x = x.squeeze(-1).permute(0, 2, 1)  # (B, 100, 32)\n",
    "\n",
    "        # Inception\n",
    "        x = self.inception(x)\n",
    "\n",
    "        # LSTM\n",
    "        t0 = time.perf_counter()\n",
    "        out, _ = self.lstm(x)\n",
    "        p(f\"    LSTM took {time.perf_counter()-t0:.4f} s – output {tuple(out.shape)}\")\n",
    "\n",
    "        y = self.head(out[:, -1])\n",
    "        p(f\"    Total forward {time.perf_counter()-t_fwd:.4f} s\")\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ae5fce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 6. Training loop with epoch/step timing and metrics\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def train(model, train_loader, val_loader,\n",
    "          epochs=40, lr=1e-3, patience_lim=5,\n",
    "          accum_steps=2):\n",
    "\n",
    "    opt    = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.999),\n",
    "                               eps=1e-8, weight_decay=1e-4, fused=True)\n",
    "    scaler = amp.GradScaler()\n",
    "    ce     = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_f1, wait = 0.0, 0\n",
    "    p(\"Starting training …\")\n",
    "    for ep in range(1, epochs + 1):\n",
    "        ep_t0 = time.perf_counter()\n",
    "        p(f\"\\n===== EPOCH {ep}/{epochs} =====\")\n",
    "\n",
    "        # ---------- TRAIN ----------\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        t_train = time.perf_counter()\n",
    "        for step, (xb, yb) in enumerate(train_loader):\n",
    "            step_t0 = time.perf_counter()\n",
    "            xb = xb.to(device, non_blocking=True).contiguous(memory_format=torch.channels_last)\n",
    "            yb = yb.to(device, non_blocking=True)\n",
    "\n",
    "            with amp.autocast():\n",
    "                logits = model(xb)\n",
    "                loss   = ce(logits, yb) / accum_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            if (step + 1) % accum_steps == 0:\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            running_loss += loss.item() * accum_steps\n",
    "            p(f\"      step {step:04d} took {time.perf_counter()-step_t0:.3f} s\")\n",
    "        p(f\"  TRAIN epoch took {time.perf_counter()-t_train:.2f} s\")\n",
    "\n",
    "        # ---------- VALID ----------\n",
    "        model.eval()\n",
    "        y_true, y_pred = [], []\n",
    "        t_val = time.perf_counter()\n",
    "        with torch.no_grad(), amp.autocast():\n",
    "            for xb, yb in val_loader:\n",
    "                logits = model(xb.to(device, non_blocking=True))\n",
    "                y_true.append(yb)\n",
    "                y_pred.append(logits.argmax(1).cpu())\n",
    "        p(f\"  VALID epoch took {time.perf_counter()-t_val:.2f} s\")\n",
    "\n",
    "        macro_f1 = f1_score(torch.cat(y_true), torch.cat(y_pred), average='macro')\n",
    "        print(f\"Ep {ep:02d}  loss={running_loss/len(train_loader.sampler):.4f}  F1={macro_f1:.4f}\")\n",
    "\n",
    "        # ---------- EARLY STOP ----------\n",
    "        if macro_f1 > best_f1 + 1e-4:\n",
    "            best_f1, wait = macro_f1, 0\n",
    "            torch.save(model.state_dict(), \"best_deeplob.pt\")\n",
    "            p(\"  New best model saved.\")\n",
    "        else:\n",
    "            wait += 1\n",
    "        if wait >= patience_lim:\n",
    "            print(\"Early stop triggered – no improvement.\")\n",
    "            break\n",
    "        p(f\"EPOCH {ep} finished in {time.perf_counter()-ep_t0:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ce5a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] load_book_chunk(): symbol=BTC-USDT, range=(2025-02-26 00:00:00 … 2025-02-28 00:00:00)\n",
      "[DEBUG]   reading book_btc_usdt_20250215_20250228.parquet\n",
      "[DEBUG]     -> 924647 rows kept from this file\n",
      "[DEBUG] Concatenated 1 chunks – total rows: 924647 – took 9.49 s\n",
      "[DEBUG] LobDataset: depth=10, window=100, horizon=100, stride=5\n",
      "[DEBUG]   selected 40 feature columns for LOB levels\n",
      "[DEBUG]   starting 5‑day z‑score normalisation (window=2643840) on GPU …\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 7. Convenience harness (example run)\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_book_chunk(dt.datetime(2025, 2, 26), dt.datetime(2025, 2, 28), \"BTC-USDT\")\n",
    "\n",
    "    train_loader, val_loader = make_loaders(df,\n",
    "        depth   = 10,\n",
    "        window  = 100,\n",
    "        horizon = 100,\n",
    "        batch   = 32)\n",
    "\n",
    "    model = DeepLOB(depth=10).to(device, memory_format=torch.channels_last)\n",
    "    model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "\n",
    "    train(model, train_loader, val_loader,\n",
    "          epochs       = 40,\n",
    "          lr           = 1e-3,\n",
    "          patience_lim = 5,\n",
    "          accum_steps  = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c529abc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best_deeplob.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Cell 7 ────────────────────────────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m DeepLOB()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_deeplob.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpredict\u001b[39m(window_100x40: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-25.02/lib/python3.10/site-packages/torch/serialization.py:1425\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1423\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1425\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1427\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1429\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-25.02/lib/python3.10/site-packages/torch/serialization.py:751\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    750\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 751\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-25.02/lib/python3.10/site-packages/torch/serialization.py:732\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 732\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best_deeplob.pt'"
     ]
    }
   ],
   "source": [
    "# Cell 7 ────────────────────────────────────────────────────────────────────\n",
    "model = DeepLOB().to(device)\n",
    "model.load_state_dict(torch.load(\"best_deeplob.pt\"))\n",
    "model.eval()\n",
    "\n",
    "def predict(window_100x40: np.ndarray) -> np.ndarray:\n",
    "    x = torch.from_numpy(window_100x40.astype(np.float32)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad(), amp.autocast():\n",
    "        logits = model(x)\n",
    "    return torch.softmax(logits, 1).cpu().numpy()[0]   # (p_down, p_stationary, p_up)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783c205c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
